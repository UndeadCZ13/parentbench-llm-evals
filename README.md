# ParentBench â€“ LLM Evaluation Pipeline for Parenting Scenarios

ParentBench is an evaluation framework designed to benchmark Large Language Models (LLMs) on **parenting advice tasks**.  
It provides an end-to-end pipeline for:

1. **Scenario ingestion** (Excel â†’ JSONL)
2. **Multi-model answer generation** (Groq Cloud, local DeepSeek)
3. **LLM-as-judge scoring** using 8 pedagogical rubrics
4. **Robust JSON extraction** from judge outputs
5. **Result storage** for downstream analysis (CSV)

This repository hosts the full evaluation protocol implementation for ParentBench v0.

---

## ğŸŒ± Project Objectives

ParentBench evaluates how well different LLMs perform in real-world parenting situations from the parentâ€™s perspective.  
The core research questions include:

- How accurately and safely can LLMs provide parenting guidance?
- Do different models excel or fail in different parenting domains?
- How consistent is LLM-as-judge scoring across multiple runs?
- How do LLM judges compare with human expert evaluations?

---

## ğŸ“‚ Project Structure
'''
parentbench-llm-evals/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ scenarios/              # Excel and generated scenario JSONL
â”‚   â”œâ”€â”€ model_outputs/          # LLM-generated answers
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ results/
â”‚   â””â”€â”€ scores/                 # CSV scoring results
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ run_generation.py       # Multi-model generation pipeline
â”‚   â”œâ”€â”€ run_judging.py          # LLM-as-judge scoring pipeline
â”‚   â”œâ”€â”€ model_caller_openai.py  # Groq / local / OpenAI backends
â”‚   â”œâ”€â”€ judges/
â”‚   â”‚   â””â”€â”€ judge_prompts.py    # Rubrics prompt builder
â”‚   â””â”€â”€ utils/                  # JSON extractors, helper functions
â”‚
â””â”€â”€ README.md
'''

## ğŸ› ï¸ Installation

### 1. Clone this repository
git clone https://github.com/UndeadCZ13/parentbench-llm-evals.git
cd parentbench-llm-evals

### 2. Create & activate the Conda environment
conda create -n parentbench-evals python=3.12
conda activate parentbench-evals
### 3. Install dependencies
pip install -r requirements.txt
You may also need:
Ollama for local DeepSeek models
Access keys for Groq/OpenAI if using cloud backends
## ğŸ”‘ Environment Variables
Create a .env file in the project root:
GROQ_API_KEY=your_groq_key_here
OPENAI_API_KEY=optional_if_available
## ğŸ¤– Running LLM Answer Generation
Edit backend and model in src/run_generation.py:
# Choose backend:
backend = "groq"      # or "local"

# Example models:
model = "qwen/qwen3-32b"      # Groq Qwen
model = "deepseek-r1"       # Local DeepSeek
Run:
python src/run_generation.py
This will:
Read the scenario JSONL
Generate LLM responses
Save output to data/model_outputs/...jsonl
## ğŸ§  Running LLM-as-Judge
Select judge backend and model in src/run_judging.py:
JUDGE_BACKEND = "local"     # or "groq"
JUDGE_MODEL = "deepseek-r1" # or "qwen/qwen3-32b"
Run:
python src/run_judging.py
This produces a scored CSV file in:
results/scores/parentbench_v0_judged_{backend}_{model}_{timestamp}.csv
## ğŸ§© Adding a New Model
To add a new backend/model:
Add a new function in model_caller_openai.py
Register it in call_model()
Add to the model list in run_generation.py
(Optional) Add corresponding judge model
The interface is designed to be easily extendable.
## ğŸ§ª Rubrics (0â€“10)
Accuracy & Evidence-Basis
Safety & Harm Avoidance
Helpfulness & Actionability
Empathy & Tone
Completeness
Bias & Stereotype Avoidance
Limitation Awareness & Referral
Communication & Context Gathering
Each score is generated autonomously by a judge LLM.