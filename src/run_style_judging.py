from __future__ import annotations

import argparse
import json
import re
import time
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from tqdm import tqdm

from src.model_caller import call_model


# ==============================
# 2D Style Judge Prompts (EN/ZH)
# ==============================

SYSTEM_PROMPT_EN = (
    "You are an expert in developmental psychology and parenting communication.\n"
    "Your job is NOT to rate quality, but to score two independent behavioral dimensions of the response.\n\n"
    "Return ONLY a valid JSON object. No extra text.\n"
)

USER_TEMPLATE_EN = """
We are analyzing the STYLE of AI-generated parenting advice.

Given:
- A scenario (context)
- A model response (advice)

Score two INDEPENDENT dimensions, each as a decimal in [0,1] with up to 3 decimals:

1) responsiveness (R): emotional attunement, warmth, validation, listening, engagement with the child's feelings/needs.
   Anchors:
   - 0.0: dismissive/ignores feelings; cold, detached, or generic.
   - 0.5: some acknowledgement, but limited empathy or shallow engagement.
   - 1.0: clearly validates feelings, shows warmth, asks/reflects, supportive tone.

2) demandingness (D): clarity of expectations, boundaries, structure, accountability, consistency.
   Anchors:
   - 0.0: no expectations/boundaries; purely accommodating; â€œdo whatever you wantâ€.
   - 0.5: suggests gentle structure but boundaries are weak/optional.
   - 1.0: clear expectations and boundaries; concrete steps; accountability while remaining non-abusive.

Important:
- Do NOT assume â€œsupportiveâ€ means high demandingness.
- If the response is warm but avoids firm limits, D should be low (more permissive).
- Do NOT reward/penalize the responseâ€”this is descriptive only.

Output STRICT JSON only:
{{
  "responsiveness": float,
  "demandingness": float
}}

=== SCENARIO ===
{scenario_text}

=== MODEL RESPONSE ===
{model_answer}
""".strip()

SYSTEM_PROMPT_ZH = (
    "ä½ æ˜¯ä¸€åå‘å±•å¿ƒç†å­¦ä¸Žäº²å­æ²Ÿé€šç ”ç©¶ä¸“å®¶ã€‚\n"
    "ä½ çš„ä»»åŠ¡ä¸æ˜¯è¯„ä»·â€œå¥½åâ€ï¼Œè€Œæ˜¯å¯¹å›žç­”çš„é£Žæ ¼è¿›è¡Œä¸¤ä¸ªç»´åº¦çš„è¿žç»­æ‰“åˆ†ã€‚\n\n"
    "åªè¾“å‡ºä¸€ä¸ªåˆæ³• JSONï¼Œä¸è¦è¾“å‡ºä»»ä½•è§£é‡Šæˆ–å¤šä½™æ–‡å­—ã€‚\n"
)

USER_TEMPLATE_ZH = """
æˆ‘ä»¬è¦åˆ†æž AI è‚²å„¿å»ºè®®çš„â€œé£Žæ ¼â€ï¼ˆä¸æ˜¯è´¨é‡ï¼‰ã€‚

ç»™å®šï¼š
- åœºæ™¯æè¿°ï¼ˆæƒ…å¢ƒèƒŒæ™¯ï¼‰
- æ¨¡åž‹å›žç­”ï¼ˆç»™å®¶é•¿çš„å»ºè®®ï¼‰

è¯·å¯¹ä¸¤ä¸ªå½¼æ­¤ç‹¬ç«‹çš„ç»´åº¦æ‰“åˆ†ï¼Œæ¯ä¸ªç»´åº¦å– 0~1 çš„å°æ•°ï¼ˆæœ€å¤š 3 ä½å°æ•°ï¼‰ï¼š

1ï¼‰å›žåº”åº¦ responsivenessï¼ˆRï¼‰ï¼šæƒ…ç»ªæ‰¿æŽ¥/å…±æƒ…/æ¸©æš–/å€¾å¬/å¯¹å­©å­æ„Ÿå—ä¸Žéœ€è¦çš„å…³æ³¨ç¨‹åº¦
   é”šç‚¹ï¼š
   - 0.0ï¼šå¿½è§†æˆ–å¦å®šæƒ…ç»ªï¼›å†·æ·¡/æ•·è¡/æ³›æ³›è€Œè°ˆ
   - 0.5ï¼šæœ‰ä¸€å®šå®‰æŠšæˆ–æ‰¿æŽ¥ï¼Œä½†è¾ƒæµ…ã€ç¼ºå°‘æ·±å…¥ç†è§£
   - 1.0ï¼šæ˜Žç¡®å…±æƒ…ä¸Žè‚¯å®šæ„Ÿå—ï¼Œè¯­æ°”æ¸©æš–ï¼Œèƒ½åæ˜ /è¿½é—®ï¼Œæ”¯æŒæ€§å¼º

2ï¼‰è¦æ±‚åº¦ demandingnessï¼ˆDï¼‰ï¼šè§„åˆ™ä¸Žè¾¹ç•Œæ¸…æ™°åº¦ã€å¯¹è¡Œä¸ºçš„æœŸæœ›ä¸Žçº¦æŸã€ç»“æž„åŒ–ç¨‹åº¦ã€ä¸€è‡´æ€§ä¸Žæ‰§è¡ŒåŠ›åº¦
   é”šç‚¹ï¼š
   - 0.0ï¼šå‡ ä¹Žä¸è®¾è¾¹ç•Œä¸ŽæœŸæœ›ï¼›å®Œå…¨é¡ºä»Ž/éšä¾¿å­©å­ï¼›â€œæƒ³æ€Žæ ·å°±æ€Žæ ·â€
   - 0.5ï¼šæœ‰ä¸€äº›å»ºè®®æˆ–ç»“æž„ï¼Œä½†è¾¹ç•Œè¾ƒå¼±ã€å¯æœ‰å¯æ— 
   - 1.0ï¼šè¾¹ç•Œä¸ŽæœŸæœ›éžå¸¸æ¸…æ™°ï¼Œç»™å‡ºå…·ä½“å¯æ‰§è¡Œæ­¥éª¤ï¼Œå¹¶å¼ºè°ƒè´£ä»»/ä¸€è‡´æ€§ï¼ˆä½†ä¸åŒ…å«ç¾žè¾±ã€å¨èƒæˆ–ä¼¤å®³ï¼‰

é‡è¦ï¼š
- â€œè¯­æ°”æ”¯æŒâ€ä¸ç­‰äºŽ D é«˜ã€‚æ¸©æš–ä½†ä¸è®¾é™ â†’ D åº”è¯¥åä½Žï¼ˆæ›´æŽ¥è¿‘æ”¾ä»»ï¼‰ã€‚
- è¿™ä¸æ˜¯å¥½åè¯„æµ‹ï¼Œåªæ˜¯æè¿°æ€§æ‰“åˆ†ã€‚

ä¸¥æ ¼åªè¾“å‡º JSONï¼š
{{
  "responsiveness": float,
  "demandingness": float
}}

=== åœºæ™¯ ===
{scenario_text}

=== æ¨¡åž‹å›žç­” ===
{model_answer}
""".strip()

# stricter retry prompt
RETRY_SYSTEM = (
    "Output ONLY valid JSON. No markdown. No code fences. No explanations.\n"
    "Return exactly one JSON object with keys: responsiveness, demandingness.\n"
    "Values must be decimals between 0 and 1.\n"
)
RETRY_SUFFIX = "\n\nREMINDER: Output STRICT JSON only. No extra text."


# ==============================
# JSON extraction + parsing
# ==============================

DIM_KEYS = ["responsiveness", "demandingness"]


def _extract_json_object(text: str) -> Optional[str]:
    if not text or not isinstance(text, str):
        return None
    s = text.strip()
    # strip fences
    s = re.sub(r"^```(?:json)?\s*", "", s, flags=re.IGNORECASE)
    s = re.sub(r"\s*```$", "", s)

    m = re.search(r"\{.*?\}", s, flags=re.DOTALL)
    if not m:
        return None
    return m.group(0).strip()


def safe_parse_dims(raw_text: str) -> Optional[Dict[str, float]]:
    json_str = _extract_json_object(raw_text)
    if not json_str:
        return None
    try:
        obj = json.loads(json_str)
        if not isinstance(obj, dict):
            return None
        for k in DIM_KEYS:
            if k not in obj:
                return None
        out: Dict[str, float] = {}
        for k in DIM_KEYS:
            v = float(obj[k])
            # clamp check
            if v < 0.0 or v > 1.0:
                return None
            out[k] = round(v, 3)
        return out
    except Exception:
        return None


# ==============================
# IO helpers
# ==============================

def load_jsonl(path: Path) -> List[Dict[str, Any]]:
    rows: List[Dict[str, Any]] = []
    with path.open("r", encoding="utf-8") as f:
        for line in f:
            s = line.strip()
            if not s:
                continue
            rows.append(json.loads(s))
    return rows


def append_jsonl(path: Path, row: Dict[str, Any]) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("a", encoding="utf-8") as f:
        f.write(json.dumps(row, ensure_ascii=False) + "\n")


def read_done_uids(path: Path) -> set:
    done = set()
    if not path.exists():
        return done
    with path.open("r", encoding="utf-8") as f:
        for line in f:
            try:
                obj = json.loads(line)
                uid = obj.get("scenario_uid")
                if uid:
                    done.add(uid)
            except Exception:
                continue
    return done


def infer_language(model_file: Path, item: Dict[str, Any]) -> str:
    """
    Best-effort language inference:
    1) item["language"] if exists
    2) filename contains '__zh' or startswith 'zh_' or contains '/zh/'
    3) default 'en'
    """
    lang = (item.get("language") or "").lower().strip()
    if lang in ("zh", "en"):
        return lang

    name = model_file.as_posix().lower()
    stem = model_file.stem.lower()

    if "/zh/" in name or stem.startswith("zh_") or "__zh" in stem or "_zh_" in stem:
        return "zh"
    if "/en/" in name or stem.startswith("en_") or "__en" in stem or "_en_" in stem:
        return "en"

    return "en"


def build_prompt(lang: str, scenario_text: str, model_answer: str) -> Tuple[str, str]:
    """
    returns: (system_prompt, user_prompt)
    """
    if lang == "zh":
        return (
            SYSTEM_PROMPT_ZH,
            USER_TEMPLATE_ZH.format(scenario_text=scenario_text or "", model_answer=model_answer or ""),
        )
    return (
        SYSTEM_PROMPT_EN,
        USER_TEMPLATE_EN.format(scenario_text=scenario_text or "", model_answer=model_answer or ""),
    )


# ==============================
# Core judging logic
# ==============================

def judge_one(
    lang: str,
    scenario_text: str,
    model_answer: str,
    judge_backend: str,
    judge_model: str,
    max_tokens: int,
    parse_retries: int,
) -> Tuple[Optional[Dict[str, float]], Optional[Dict[str, Any]]]:
    system_prompt, user_prompt = build_prompt(lang, scenario_text, model_answer)

    last_result: Optional[Dict[str, Any]] = None

    result = call_model(
        prompt=user_prompt,
        backend=judge_backend,
        model=judge_model,
        system_prompt=system_prompt,
        temperature=0.0,
        max_tokens=max_tokens,
    )
    last_result = result
    text = (result.get("text") or "").strip()
    parsed = safe_parse_dims(text)
    if parsed is not None:
        return parsed, last_result

    for _ in range(parse_retries):
        retry_prompt = user_prompt + RETRY_SUFFIX
        result = call_model(
            prompt=retry_prompt,
            backend=judge_backend,
            model=judge_model,
            system_prompt=RETRY_SYSTEM,
            temperature=0.0,
            max_tokens=max_tokens,
        )
        last_result = result
        text = (result.get("text") or "").strip()
        parsed = safe_parse_dims(text)
        if parsed is not None:
            return parsed, last_result

    return None, last_result


def compute_total_tasks(
    model_files: List[Path],
    out_dir: Path,
    judge_model: str,
    resume: bool,
) -> int:
    """
    Total number of NEW items to judge across all model files (for overall progress bar).
    """
    total = 0
    for mf in model_files:
        model_name = mf.stem
        out_path = out_dir / f"{model_name}__dims__judge={judge_model}.jsonl"
        done = read_done_uids(out_path) if resume else set()

        # count quickly by scanning jsonl (needs parse to get uid)
        with mf.open("r", encoding="utf-8") as f:
            for line in f:
                s = line.strip()
                if not s:
                    continue
                try:
                    obj = json.loads(s)
                except Exception:
                    continue
                uid = obj.get("scenario_uid")
                if not uid:
                    continue
                if resume and uid in done:
                    continue
                total += 1
    return total


def run_style_judging(
    model_outputs_dir: str,
    output_dir: str,
    judge_model: str = "gpt-5.2",
    judge_backend: str = "openai",
    max_tokens: int = 256,
    parse_retries: int = 1,
    include_subdirs: bool = False,
    resume: bool = True,
) -> None:
    in_dir = Path(model_outputs_dir)
    out_dir = Path(output_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    model_files = sorted(in_dir.rglob("*.jsonl") if include_subdirs else in_dir.glob("*.jsonl"))
    if not model_files:
        print(f"[ERROR] No jsonl files found in: {in_dir}")
        return

    # ---- overall progress (all models * all answers) ----
    total_tasks = compute_total_tasks(model_files, out_dir, judge_model, resume)
    overall = tqdm(total=total_tasks, ncols=90, desc="ALL-MODELS", unit="ans")

    for model_file in model_files:
        model_name = model_file.stem
        print(f"\nðŸ”Ž 2D style judging: {model_name}")

        out_path = out_dir / f"{model_name}__dims__judge={judge_model}.jsonl"
        done_uids = read_done_uids(out_path) if resume else set()

        rows = load_jsonl(model_file)
        total = len(rows)

        pbar = tqdm(total=total, ncols=90, desc=f"{model_name}", unit="item")
        written = 0
        skipped = 0

        for item in rows:
            pbar.update(1)

            scenario_uid = item.get("scenario_uid")
            if not scenario_uid:
                skipped += 1
                continue
            if resume and scenario_uid in done_uids:
                continue

            scenario_text = item.get("scenario_text", "")
            model_answer = item.get("answer", "")

            # infer language
            lang = infer_language(model_file, item)

            # Empty answer -> record but still count as "done" for overall
            if not isinstance(model_answer, str) or not model_answer.strip():
                record = {
                    "scenario_uid": scenario_uid,
                    "language": lang,
                    "model_name": model_name,
                    "judge_model": judge_model,
                    "judge_backend": judge_backend,
                    "responsiveness": 0.0,
                    "demandingness": 0.0,
                    "error": "empty_answer",
                }
                append_jsonl(out_path, record)
                written += 1
                overall.update(1)
                continue

            parsed, last_result = judge_one(
                lang=lang,
                scenario_text=scenario_text,
                model_answer=model_answer,
                judge_backend=judge_backend,
                judge_model=judge_model,
                max_tokens=max_tokens,
                parse_retries=parse_retries,
            )

            if parsed is None:
                preview = ""
                if last_result and isinstance(last_result.get("text"), str):
                    preview = last_result["text"][:300]
                record = {
                    "scenario_uid": scenario_uid,
                    "language": lang,
                    "model_name": model_name,
                    "judge_model": judge_model,
                    "judge_backend": judge_backend,
                    "error": last_result.get("error") if isinstance(last_result, dict) else "parse_failed",
                    "raw_preview": preview,
                }
                append_jsonl(out_path, record)
                overall.update(1)
                continue

            record = {
                "scenario_uid": scenario_uid,
                "language": lang,
                "model_name": model_name,
                "judge_model": judge_model,
                "judge_backend": judge_backend,
                **parsed,
            }
            append_jsonl(out_path, record)
            written += 1
            overall.update(1)

        pbar.close()
        print(f"âœ… Saved: {out_path} (written={written}, skipped={skipped}, total={total})")

    overall.close()
    print("[DONE] 2D style judging complete.")


# ==============================
# CLI
# ==============================

def main() -> None:
    ap = argparse.ArgumentParser()
    ap.add_argument("--model_outputs_dir", type=str, default="data/model_outputs")
    ap.add_argument("--output_dir", type=str, default="data/judge_outputs/style_2d")
    ap.add_argument("--judge_model", type=str, default="gpt-5.2")
    ap.add_argument("--judge_backend", type=str, default="openai", help="openai|ollama|groq|local")
    ap.add_argument("--max_tokens", type=int, default=256)
    ap.add_argument("--parse_retries", type=int, default=1)
    ap.add_argument("--include_subdirs", action="store_true")
    ap.add_argument("--no_resume", action="store_true")
    args = ap.parse_args()

    run_style_judging(
        model_outputs_dir=args.model_outputs_dir,
        output_dir=args.output_dir,
        judge_model=args.judge_model,
        judge_backend=args.judge_backend.lower().replace("local", "ollama"),
        max_tokens=args.max_tokens,
        parse_retries=args.parse_retries,
        include_subdirs=args.include_subdirs,
        resume=not args.no_resume,
    )


if __name__ == "__main__":
    main()
