"""
merge_judge_scores.py

ç”¨é€”ï¼š
  - å°†å¤šä¸ª judge ç»“æœ CSV åˆå¹¶ä¸ºä¸€ä¸ª all_judge_scores.csv
  - è‡ªåŠ¨æ£€æŸ¥ rubrics åˆ—æ˜¯å¦ä¸€è‡´ï¼Œé¿å…æ··å…¥æ—§ç‰ˆæœ¬æ–‡ä»¶
  - åœ¨å¿…è¦æ—¶å¯¹ç¼ºå¤±çš„ rubrics è¡¥åˆ—ï¼ˆå¡« NaNï¼‰ï¼Œå¹¶æ‰“å°è­¦å‘Š
  - ä¸ºæ¯æ¡è®°å½•å¢åŠ  source_file åˆ—ï¼Œæ–¹ä¾¿è¿½æº¯

ä½¿ç”¨æ–¹å¼ï¼š
  1. ä¿®æ”¹ INPUT_FILES åˆ—è¡¨ï¼Œå¡«å…¥ä½ æƒ³åˆå¹¶çš„ judge CSV æ–‡ä»¶è·¯å¾„
  2. åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œï¼š
       python -m src.analysis.merge_judge_scores
     æˆ–ï¼š
       cd src && python analysis/merge_judge_scores.py
"""

from pathlib import Path
import pandas as pd

# ======== éœ€è¦ä½ æ‰‹åŠ¨å¡«å†™çš„éƒ¨åˆ†ï¼šè¦åˆå¹¶çš„ judge æ–‡ä»¶åˆ—è¡¨ ========
INPUT_FILES = [
    # ç¤ºä¾‹ï¼ˆè¯·æ”¹æˆä½ è‡ªå·±çš„æ–‡ä»¶è·¯å¾„ï¼‰ï¼š
    "results/scores/parentbench_v0_openai_gpt-4o-mini_20251203-220722_judged_openai_gpt-4o-mini.csv",
    "results/scores/parentbench_v0_local_deepseek-r1_20251204-171641_judged_openai_gpt-4o-mini.csv",
    "results/scores/parentbench_v0_groq_qwen-qwen3-32b_20251116-170212_judged_openai_gpt-4o-mini.csv",
    "results/scores/parentbench_v0_ollama_gpt-oss-20b-cloud_20251205-143035_judged_openai_gpt-4o-mini.csv",
    "results/scores/parentbench_v0_ollama_deepseek-v3.1-671b-cloud_20251205-145354_judged_openai_gpt-4o-mini.csv",
    "results/scores/parentbench_v0_ollama_kimi-k2_20251205-162446_judged_openai_gpt-4o-mini.csv",
    "results/scores/parentbench_v0_ollama_ministral3_8b_20251205-173601_judged_openai_gpt-4o-mini.csv",
    "results/scores/parentbench_v0_ollama_ministral3_14b_20251205-173601_judged_openai_gpt-4o-mini.csv",
    "results/scores/parentbench_v0_ollama_minimax_m2_20251205-173601_judged_openai_gpt-4o-mini.csv",
]

# è¾“å‡ºæ–‡ä»¶è·¯å¾„
OUTPUT_DIR = Path("results/merged")
OUTPUT_FILE = OUTPUT_DIR / "all_judge_scores.csv"

# ======== å¦‚æœä½ æœ‰ç»Ÿä¸€çš„ rubric åˆ—åï¼Œå¯ä»¥åœ¨è¿™é‡Œå†™æ­» ========
# å¦‚æœä½ å·²ç»åœ¨ judges/judge_prompts.py é‡Œå®šä¹‰äº† RUBRIC_KEYSï¼Œä¹Ÿå¯ä»¥ import
# from judges.judge_prompts import RUBRIC_KEYS
RUBRIC_KEYS = [
    "accuracy",
    "safety",
    "helpfulness",
    "empathy",
    "completeness",
    "bias_avoidance",
    "limitation_awareness",
    "communication",
]

# å…¶ä»–â€œå…ƒä¿¡æ¯â€åˆ—ï¼ˆå¦‚æœä½ æœ‰åˆ«çš„ï¼Œä¹Ÿå¯ä»¥åŠ è¿›æ¥ï¼‰
META_COLUMNS_CANDIDATES = [
    "scenario_id",
    "backend_answer",    # ç”Ÿæˆå›ç­”ç”¨çš„ backendï¼ˆopenai/local/groqç­‰ï¼‰
    "model_answer",      # ç”Ÿæˆå›ç­”çš„æ¨¡å‹å
    "judge_backend",
    "judge_model",
    "generated_at",
    "comment",
]


def main():
    if not INPUT_FILES:
        raise ValueError("è¯·å…ˆåœ¨ INPUT_FILES ä¸­å¡«å…¥è‡³å°‘ä¸€ä¸ª judge ç»“æœ CSV æ–‡ä»¶è·¯å¾„ã€‚")

    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

    all_dfs = []
    all_rubric_sets = []

    print("å³å°†åˆå¹¶ä»¥ä¸‹æ–‡ä»¶ï¼š")
    for fp in INPUT_FILES:
        print("  -", fp)
    print()

    for fp in INPUT_FILES:
        path = Path(fp)
        if not path.exists():
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ–‡ä»¶ï¼š{path}")

        df = pd.read_csv(path)

        # è®°å½•æ¥æºæ–‡ä»¶å
        df["source_file"] = path.name

        # æ£€æŸ¥æœ‰å“ªäº› rubrics åˆ—
        rubric_cols_in_file = [col for col in df.columns if col in RUBRIC_KEYS]
        all_rubric_sets.append(set(rubric_cols_in_file))

        # æé†’å¯èƒ½ç¼ºå¤±ï¼å¤šå‡ºçš„åˆ—
        missing_rubrics = [r for r in RUBRIC_KEYS if r not in rubric_cols_in_file]
        extra_rubrics = [col for col in rubric_cols_in_file if col not in RUBRIC_KEYS]

        print(f"æ–‡ä»¶ {path.name}:")
        print(f"  å‘ç°çš„ rubric åˆ—ï¼š{rubric_cols_in_file}")

        if missing_rubrics:
            print(f"  âš  ç¼ºå¤±çš„ rubric åˆ—ï¼š{missing_rubrics}ï¼ˆå°†åœ¨åˆå¹¶æ—¶è¡¥å……ä¸ºç©ºå€¼ï¼‰")
        if extra_rubrics:
            print(f"  âš  é¢å¤–çš„ rubric åˆ—ï¼ˆæœªåœ¨ RUBRIC_KEYS ä¸­ï¼‰ï¼š{extra_rubrics}")

        # å¯¹ç¼ºå¤±çš„ rubrics è¡¥åˆ—
        for r in RUBRIC_KEYS:
            if r not in df.columns:
                df[r] = pd.NA

        all_dfs.append(df)
        print()

    # æ£€æŸ¥æ‰€æœ‰æ–‡ä»¶çš„ rubric é›†åˆæ˜¯å¦ä¸€è‡´ï¼ˆä»…ä½œæç¤ºï¼Œä¸å¼ºåˆ¶æŠ¥é”™ï¼‰
    unique_rubric_sets = {tuple(sorted(s)) for s in all_rubric_sets}
    if len(unique_rubric_sets) > 1:
        print("ğŸ” æ³¨æ„ï¼šä¸åŒæ–‡ä»¶çš„ rubrics åˆ—é›†åˆä¸å®Œå…¨ä¸€è‡´ï¼Œå·²é€šè¿‡è¡¥åˆ—æ–¹å¼å¯¹é½ã€‚")
    else:
        print("âœ… æ‰€æœ‰æ–‡ä»¶çš„ rubrics åˆ—é›†åˆä¸€è‡´ã€‚")

    # åˆå¹¶
    merged = pd.concat(all_dfs, ignore_index=True)

    # ç»Ÿä¸€åˆ—é¡ºåºï¼ˆä¾¿äºåç»­åˆ†æï¼‰
    # å…ˆæŠŠå¸¸è§çš„ meta åˆ—æ”¾å‰é¢ï¼Œå†æ˜¯ rubricsï¼Œå‰©ä¸‹çš„æ”¾åé¢
    meta_cols_present = [c for c in META_COLUMNS_CANDIDATES if c in merged.columns]
    cols_order = meta_cols_present + RUBRIC_KEYS

    # æŠŠå‰©ä½™çš„åˆ—ï¼ˆä¸åœ¨ meta + rubrics é‡Œçš„ï¼‰è¿½åŠ åˆ°æœ€å
    remaining_cols = [c for c in merged.columns if c not in cols_order]
    cols_order += remaining_cols

    merged = merged[cols_order]

    # ä¿å­˜
    merged.to_csv(OUTPUT_FILE, index=False, encoding="utf-8")
    print("\nâœ… åˆå¹¶å®Œæˆã€‚")
    print(f"  å…±åˆå¹¶ {len(INPUT_FILES)} ä¸ªæ–‡ä»¶ï¼Œå¾—åˆ° {len(merged)} æ¡è®°å½•ã€‚")
    print(f"  è¾“å‡ºæ–‡ä»¶ï¼š{OUTPUT_FILE.resolve()}")


if __name__ == "__main__":
    main()
